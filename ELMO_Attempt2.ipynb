{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELMO_Attempt2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPRqMFDHn3Wl",
        "colab_type": "text"
      },
      "source": [
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDaGPe4jn3Wm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "##from pattern import en\n",
        "from scipy import spatial\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XqMAj9qxHiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##!pip install pattern"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag0dFHFUn3Wr",
        "colab_type": "text"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W68ZAn1pn3Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_repository = 'candidates_2019_05_24_16_24.csv'\n",
        "##job_repository = 'indeed_job_dataset_1.csv'\n",
        "##word2vecModel = 'resume_word2vec'\n",
        "##word2vecResume = 'resume_w2v_array'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBIdhPVvxxVc",
        "colab_type": "code",
        "outputId": "6302c819-536c-42cb-b8cb-e54587aa6539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5LaI2Udzzrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path ='/content/drive/My Drive/Capstone/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S1c9KzO0G1R",
        "colab_type": "code",
        "outputId": "d90ae5ce-ba8d-4ca4-b276-800018d20260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88-9SIeA0T-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmJ8Yv5h0JNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(project_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdNLfAfF0a9v",
        "colab_type": "code",
        "outputId": "74b0389f-b9aa-4558-ac45-80d497fe4025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Capstone'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lMwLhFon3Wu",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6q4YL1Qn3Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_resume_data():\n",
        "    dfdata = pd.read_csv(data_repository)\n",
        "    dfdata['whole_text'] = pd.Series(dfdata.fillna('').values.tolist()).str.join(' ')\n",
        "    return dfdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjG24VJyn3Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_jobprofile_data():\n",
        "    dfjob = pd.read_csv(job_repository)\n",
        "    #dfjob['whole_text'] = pd.Series(dfjob.fillna('').values.tolist()).str.join(' ')\n",
        "    return dfjob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd7_sSctn3W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_w2v_model(dataset):    \n",
        "    alltext = ' '  \n",
        "    for index, row in dataset.iterrows():\n",
        "        alltext = alltext + \" \" + row['whole_text']   \n",
        "    alltext = alltext.lower()\n",
        "    vector = []\n",
        "    for sentence in en.parsetree(alltext, tokenize=True, lemmata=True, tags=True):\n",
        "        temp = []\n",
        "        for chunk in sentence.chunks:\n",
        "            for word in chunk.words:\n",
        "                if word.tag == 'NN' or word.tag == 'VB':\n",
        "                    temp.append(word.lemma)\n",
        "        vector.append(temp)\n",
        "    model = Word2Vec(vector, size=200, window=5, min_count=3, workers=4)\n",
        "    model.save(word2vecModel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayBaeakGn3W3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_job_desc(skill, description):\n",
        "    vector = []\n",
        "    for sentence in en.parsetree(description, tokenize=True, lemmata=True, tags=True):\n",
        "        for chunk in sentence.chunks:\n",
        "            for word in chunk.words:\n",
        "                if word.tag == 'NN' or word.tag == 'VB':\n",
        "                    vector.append(word.lemma)\n",
        "    for sentence in en.parsetree(skill, tokenize=True, lemmata=True, tags=True):\n",
        "        for chunk in sentence.chunks:\n",
        "            for word in chunk.words:\n",
        "                vector.append(word.lemma)\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbECz88Gn3W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_w2v_resume(df_resume):\n",
        "    D_w2v = []\n",
        "    for index, row in df_resume.iterrows():\n",
        "        print(\"Processing resume \" + str(index))\n",
        "        yd = row['whole_text']\n",
        "        w2v = []\n",
        "        for sentence in en.parsetree(yd.lower(), tokenize=True, lemmata=True, tags=True):\n",
        "            for chunk in sentence.chunks:\n",
        "                for word in chunk.words:\n",
        "                    if word.lemma in model.wv.vocab:\n",
        "                        w2v.append(model.wv[word.lemma])\n",
        "                    else:\n",
        "                        if word.lemma.lower() in model.wv.vocab:\n",
        "                            w2v.append(model.wv[word.lemma.lower()])\n",
        "        D_w2v.append((np.mean(w2v, axis=0),index))\n",
        "    with open(word2vecResume, 'wb') as fp:\n",
        "        pickle.dump(D_w2v, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ertALn90n3W8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_recommend_initialrun(job_profile, df_resume):  \n",
        "    #data = request.args.get('value')\n",
        "    w2v = []\n",
        "    job_profile = job_profile.lower()\n",
        "    model = Word2Vec.load(word2vecModel)\n",
        "    for sentence in en.parsetree(job_profile, tokenize=True, lemmata=True, tags=True):\n",
        "        for chunk in sentence.chunks:\n",
        "            for word in chunk.words:\n",
        "                if word.lemma in model.wv.vocab:\n",
        "                    w2v.append(model.wv[word.lemma])\n",
        "                else:\n",
        "                    if word.lemma.lower() in model.wv.vocab:\n",
        "                        w2v.append(model.wv[word.lemma.lower()])\n",
        "    Q_w2v = np.mean(w2v, axis=0)\n",
        "    \n",
        "    print(\"completed job profile screening\")\n",
        "    \n",
        "    # Example of document represented by average of each document term vectors.\n",
        "    D_w2v = []\n",
        "    for index, row in df_resume.iterrows():\n",
        "        print(\"Processing resume \" + str(index))\n",
        "        yd = row['whole_text']\n",
        "        w2v = []\n",
        "        for sentence in en.parsetree(yd.lower(), tokenize=True, lemmata=True, tags=True):\n",
        "            for chunk in sentence.chunks:\n",
        "                for word in chunk.words:\n",
        "                    if word.lemma in model.wv.vocab:\n",
        "                        w2v.append(model.wv[word.lemma])\n",
        "                    else:\n",
        "                        if word.lemma.lower() in model.wv.vocab:\n",
        "                            w2v.append(model.wv[word.lemma.lower()])\n",
        "        D_w2v.append((np.mean(w2v, axis=0),index))\n",
        "    with open(word2vecResume, 'wb') as fp:\n",
        "        pickle.dump(D_w2v, fp)\n",
        "    \n",
        "    # Make the retrieval using cosine similarity between query and document vectors.\n",
        "    retrieval = []\n",
        "    for i in range(len(D_w2v)):\n",
        "        print('Calulating cosine similarity for resume: ' + str(i))\n",
        "        retrieval.append((1 - spatial.distance.cosine(Q_w2v, D_w2v[i][0]),D_w2v[i][1]))\n",
        "    retrieval.sort(reverse=True)\n",
        "    return retrieval\n",
        "    #with app.app_context(), app.test_request_context():\n",
        "        #ret_data = {\"cv1\":url_for('static', filename=\"test/\"+retrieval[0][1][retrieval[0][1].rfind('/')+1:]), \"score1\": str(round(retrieval[0][0], 4)), \"cv2\":url_for('static', filename=\"test/\"+retrieval[1][1][retrieval[1][1].rfind('/')+1:]), \"score2\": str(round(retrieval[1][0], 4)),\"cv3\":url_for('static', filename=\"test/\"+retrieval[2][1][retrieval[2][1].rfind('/')+1:]), \"score3\": str(round(retrieval[2][0], 4))   }\n",
        "        #return jsonify(ret_data)\n",
        "        #return ret_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYeIgc6n3W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_recommend(job_profile, df_resume):  \n",
        "    #data = request.args.get('value')\n",
        "    w2v = []\n",
        "    job_profile = job_profile.lower()\n",
        "    model = Word2Vec.load(word2vecModel)\n",
        "    for sentence in en.parsetree(job_profile, tokenize=True, lemmata=True, tags=True):\n",
        "        for chunk in sentence.chunks:\n",
        "            for word in chunk.words:\n",
        "                if word.lemma in model.wv.vocab:\n",
        "                    w2v.append(model.wv[word.lemma])\n",
        "                else:\n",
        "                    if word.lemma.lower() in model.wv.vocab:\n",
        "                        w2v.append(model.wv[word.lemma.lower()])\n",
        "    Q_w2v = np.mean(w2v, axis=0)\n",
        "    \n",
        "    # Document represented by average of each document term vectors.\n",
        "    #D_w2v = []\n",
        "    with open(word2vecResume, 'rb') as fp:\n",
        "        D_w2v = pickle.load(fp)\n",
        "    \n",
        "    # Make the retrieval using cosine similarity between query and document vectors.\n",
        "    retrieval = []\n",
        "    for i in range(len(D_w2v)):\n",
        "        retrieval.append((1 - spatial.distance.cosine(Q_w2v, D_w2v[i][0]),D_w2v[i][1]))\n",
        "    retrieval.sort(reverse=True)\n",
        "    return retrieval\n",
        "    #with app.app_context(), app.test_request_context():\n",
        "        #ret_data = {\"cv1\":url_for('static', filename=\"test/\"+retrieval[0][1][retrieval[0][1].rfind('/')+1:]), \"score1\": str(round(retrieval[0][0], 4)), \"cv2\":url_for('static', filename=\"test/\"+retrieval[1][1][retrieval[1][1].rfind('/')+1:]), \"score2\": str(round(retrieval[1][0], 4)),\"cv3\":url_for('static', filename=\"test/\"+retrieval[2][1][retrieval[2][1].rfind('/')+1:]), \"score3\": str(round(retrieval[2][0], 4))   }\n",
        "        #return jsonify(ret_data)\n",
        "        #return ret_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acd_bZnLn3XB",
        "colab_type": "text"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcK6s0n6n3XC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfdata = get_resume_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2rejqNX0pb0",
        "colab_type": "code",
        "outputId": "ddbd85ca-7cf9-4160-d9df-36473b256ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "dfdata.head(2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>First Name</th>\n",
              "      <th>Last Name</th>\n",
              "      <th>Position</th>\n",
              "      <th>Company</th>\n",
              "      <th>Location</th>\n",
              "      <th>Experience1</th>\n",
              "      <th>Experience2</th>\n",
              "      <th>Experience3</th>\n",
              "      <th>Experience4</th>\n",
              "      <th>Experience5</th>\n",
              "      <th>Experience6</th>\n",
              "      <th>Experience7</th>\n",
              "      <th>Experience8</th>\n",
              "      <th>Experience9</th>\n",
              "      <th>Experience10</th>\n",
              "      <th>Experience11</th>\n",
              "      <th>Experience12</th>\n",
              "      <th>Experience13</th>\n",
              "      <th>Experience14</th>\n",
              "      <th>Experience15</th>\n",
              "      <th>Experience16</th>\n",
              "      <th>Experience17</th>\n",
              "      <th>Experience18</th>\n",
              "      <th>Experience19</th>\n",
              "      <th>Experience20</th>\n",
              "      <th>Experience21</th>\n",
              "      <th>Experience22</th>\n",
              "      <th>Experience23</th>\n",
              "      <th>Experience24</th>\n",
              "      <th>Experience25</th>\n",
              "      <th>Experience26</th>\n",
              "      <th>Experience27</th>\n",
              "      <th>Experience28</th>\n",
              "      <th>Experience29</th>\n",
              "      <th>Experience30</th>\n",
              "      <th>Experience31</th>\n",
              "      <th>Experience32</th>\n",
              "      <th>Experience33</th>\n",
              "      <th>Experience34</th>\n",
              "      <th>Experience35</th>\n",
              "      <th>Experience36</th>\n",
              "      <th>Experience37</th>\n",
              "      <th>Skill</th>\n",
              "      <th>Education1</th>\n",
              "      <th>Education2</th>\n",
              "      <th>Education3</th>\n",
              "      <th>Education4</th>\n",
              "      <th>Education5</th>\n",
              "      <th>Education6</th>\n",
              "      <th>Education7</th>\n",
              "      <th>Education8</th>\n",
              "      <th>Education9</th>\n",
              "      <th>Education10</th>\n",
              "      <th>Education11</th>\n",
              "      <th>Education12</th>\n",
              "      <th>Education13</th>\n",
              "      <th>Education14</th>\n",
              "      <th>whole_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Igor</td>\n",
              "      <td>Beliaev</td>\n",
              "      <td>Senior Manager, Data Science</td>\n",
              "      <td>Gogo</td>\n",
              "      <td>Chicago, IL, United States</td>\n",
              "      <td>Lead Data Analyst, Gogo, 2015-2016\\nMigrated m...</td>\n",
              "      <td>Senior Data Analyst, Gogo, 2013-2015\\nExtended...</td>\n",
              "      <td>Data Analyst, Gogo, 2011-2013\\nData analysis o...</td>\n",
              "      <td>Data Analyst, Market Dynamics, Inc, 2006-2011\\...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>.NET, Access, Agile Methodologies, Algorithms,...</td>\n",
              "      <td>MS, Mathematics, Computer Science, The Univers...</td>\n",
              "      <td>Assoc., Management, Banking and Finance, Briti...</td>\n",
              "      <td>BSc., Applied Mathematics, Moscow Aviation Ins...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Igor Beliaev Senior Manager, Data Science Gogo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adam</td>\n",
              "      <td>Owens</td>\n",
              "      <td>Data Scientist, Revenue</td>\n",
              "      <td>Snapchat, Inc.</td>\n",
              "      <td>Los Angeles, CA, United States</td>\n",
              "      <td>Marketing Data Analyst, Snapchat, Inc., 2017-2...</td>\n",
              "      <td>Data Analytics Instructor, Product School, 201...</td>\n",
              "      <td>Data Strategy Consultant / Founder, Vault Econ...</td>\n",
              "      <td>Data Scientist, Intern, WeWork, 2017-2017\\nBui...</td>\n",
              "      <td>Data Science, WeWork, 2017-</td>\n",
              "      <td>Technical Consultant - Marketing Analytics, Pr...</td>\n",
              "      <td>Investment Banking, Wells Fargo, 2010-2014\\nCo...</td>\n",
              "      <td>Corporate Financial Analyst, Citigroup, 2009-2...</td>\n",
              "      <td>Economics Research Assistant, Brigham Young Un...</td>\n",
              "      <td>Research Assistant Intern, Center for Strategi...</td>\n",
              "      <td>Business Strategy, Crewspark, 2015-2015\\n- Lea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Accounting, Apache Spark, Bloomberg, Business ...</td>\n",
              "      <td>Certificate / Masters Degree, Computer Science...</td>\n",
              "      <td>BA, Economics, Brigham Young University, 2005-...</td>\n",
              "      <td>Data Science Bootcamp, NYC Data Science Academ...</td>\n",
              "      <td>Classwork, Computer Science, The Johns Hopkins...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Adam Owens Data Scientist, Revenue Snapchat, I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  First Name  ...                                         whole_text\n",
              "0       Igor  ...  Igor Beliaev Senior Manager, Data Science Gogo...\n",
              "1       Adam  ...  Adam Owens Data Scientist, Revenue Snapchat, I...\n",
              "\n",
              "[2 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrikSh5sn3XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##dfjob = get_jobprofile_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvQdsPHxn3XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##create_w2v_model(dfdata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yhQx0eJn3XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = Word2Vec.load(\"resume_word2vec\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHxROSEJn3XM",
        "colab_type": "code",
        "outputId": "1e63d86c-5123-496a-e390-c7c8d9030548",
        "colab": {}
      },
      "source": [
        "#if 'matlab' in word_vectors.vocab:\n",
        " #   print(\"Yes\")\n",
        "#else:\n",
        " #   print(\"No\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1BF2zrwn3XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##job_profile = dfjob.Skill[0] + dfjob.Description[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDAMV3xNn3XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##result = get_recommend(job_profile, dfdata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR8uaVcu22Ny",
        "colab_type": "code",
        "outputId": "43e8f42e-724e-483f-9def-f25efef38065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "dfdata['whole_text'][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Igor Beliaev Senior Manager, Data Science Gogo Chicago, IL, United States Lead Data Analyst, Gogo, 2015-2016\\nMigrated machine data processing solution to Hadoop on-prem cluster, lead team of junior developers in migration of legacy applications to big data platform, building new analytical capabilities (Hadoop, hive, spark, pig, java, python) Senior Data Analyst, Gogo, 2013-2015\\nExtended scope of corporate data warehouse with operational data and built analytics and applications to support fleet maintenance and operations groups (SQL Server, SSIS, Tableau, .Net) Data Analyst, Gogo, 2011-2013\\nData analysis of airborne and ground based hardware to support operations groups troubleshooting and increase efficiency (.Net, SQL Server, MySQL, Tableau) Data Analyst, Market Dynamics, Inc, 2006-2011\\nCoordinate data research and full cycle software development between multi-cultural, geographically distributed programmers groups to support data analysis, data mining, processing and analysis algorithms design, implementation, testing, specifications for development, data processes maintenance and development, software systems and databases design, development, management, testing and maintenance.                                  .NET, Access, Agile Methodologies, Algorithms, Analysis, Apache Spark, Artificial Intelligence, AWS, Big Data, Business Analysis, Business Intelligence, Business Process Improvement, C#, Computer Science, Data Analysis, Databases, Data Mining, Data Visualization, Data Warehousing, Distributed Systems, Git, Hadoop, Integration, Java, Jira, Management, Market Research, matlab, Microsoft SQL Server, MySQL, Optimization, PostgreSQL, Programming, Project Management, Python, R, Requirements Analysis, SDLC, SharePoint, Software Development, Software Project Management, SPSS, SQL, SQL Server, T-SQL, Unix, VB.NET, Visual Studio MS, Mathematics, Computer Science, The University of Memphis, 1999-2008 Assoc., Management, Banking and Finance, British College of Banking and Finance, 1997-1999 BSc., Applied Mathematics, Moscow Aviation Institute (National Research University), 1993-1999           '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ytsS3OlmlM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "290891a4-663b-49d2-850d-4f45f8dfec9b"
      },
      "source": [
        "len(dfdata)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLLiyxJmnV-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfdata1 = dfdata.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWSqcxrTnbF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97641e0a-af9a-4b3f-c20c-ca3e92d5b461"
      },
      "source": [
        "len(dfdata1)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOyxt5li0wlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iOuMIc036A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://tfhub.dev/google/elmo/2\"\n",
        "embed = hub.Module(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ityRK08Q2sev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elmo_vectors(x):\n",
        "  embeddings = embed(x.tolist(), signature=\"default\", as_dict=True)[\"elmo\"]\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # return average of ELMo features\n",
        "    return sess.run(tf.reduce_mean(embeddings,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JytYisN2w01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_train = [elmo_vectors(dfdata1['whole_text'])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQk8j5AV3WMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71972bdf-d0c7-4ab3-9d74-95768935d9af"
      },
      "source": [
        "type(elmo_train)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8xBvyE5oKD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_train = np.asarray(elmo_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGaLloIpzmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8def896-7489-4995-d5a9-3bd6224c40ce"
      },
      "source": [
        "type(elmo_train)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_6HlYIUqZET",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "702605d4-41f6-43a4-8fc3-e004a6b033d6"
      },
      "source": [
        "elmo_train"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.09034549,  0.01817906,  0.03737507, ...,  0.03317315,\n",
              "         -0.00984211, -0.11262437],\n",
              "        [-0.13330315, -0.04385478,  0.10310254, ...,  0.03069372,\n",
              "          0.09470636, -0.12958546],\n",
              "        [-0.08792215,  0.1135233 , -0.00591619, ...,  0.00296325,\n",
              "         -0.0104468 , -0.2170866 ],\n",
              "        ...,\n",
              "        [-0.1016537 , -0.03657316,  0.06252412, ..., -0.00271141,\n",
              "         -0.01541165, -0.1655155 ],\n",
              "        [-0.1415498 ,  0.08503515, -0.0303641 , ..., -0.01843785,\n",
              "          0.0048802 , -0.19220376],\n",
              "        [-0.1348576 ,  0.03137769,  0.02285027, ...,  0.01124741,\n",
              "         -0.08620825, -0.14283521]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjDGdGDxqkYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66f5de2f-417b-48b8-83fa-5f40d0cbe579"
      },
      "source": [
        "elmo_train.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY6sNTEYqolH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nsamples, nx, ny = elmo_train.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPxh2BxTqz_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d1904cf4-b23e-4be6-8ce9-669b9391af7a"
      },
      "source": [
        "print(nsamples)\n",
        "print(nx)\n",
        "print(ny)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "20\n",
            "1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRpJsYTYq1PN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elmo_train = elmo_train.reshape((nx,nsamples*ny))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2PB0nooq61W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f3efb91-d88a-4581-d4de-010b916daa14"
      },
      "source": [
        "elmo_train.shape"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1024)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuGhjpk6rBxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "y = pca.fit_transform(elmo_train)\n",
        "## model1=pca.transform(np.array(list(model1.values())))\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "y = TSNE(n_components=3).fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eo8h8jgrNIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f6146f7-53d6-441f-839d-e898b2cdebb3"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYmU-51brfiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = dfdata1['whole_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4o1XJ7DryXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pJ17qUArOq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2140
        },
        "outputId": "41311866-274e-4ea3-a9e4-56cf5a7b5503"
      },
      "source": [
        "##@title Sementic search\n",
        "##@markdown Enter a set of words to find matching sentences. 'results_returned' can beused to modify the number of matching sentences retured. To view the code behind this cell, use the menu in the top right to unhide...\n",
        "\n",
        "search_string = \"Manager,Python\" #@param {type:\"string\"}\n",
        "results_returned = \"3\" #@param [1, 2, 3]\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "embeddings2 = embed(\n",
        "    [search_string],\n",
        "    signature=\"default\",\n",
        "    as_dict=True)[\"default\"]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  sess.run(tf.tables_initializer())\n",
        "  search_vect = sess.run(embeddings2)\n",
        "  \n",
        "\n",
        "cosine_similarities = pd.Series(cosine_similarity(search_vect, elmo_train).flatten())\n",
        "output =\"\"\n",
        "for i,j in cosine_similarities.nlargest(int(results_returned)).iteritems():\n",
        "  output +='<p style=\"font-family:verdana; font-size:110%;\"> '\n",
        "  for i in sentences[i].split():\n",
        "    if i.lower() in search_string:\n",
        "      output += \" <b>\"+str(i)+\"</b>\"\n",
        "    else:\n",
        "      output += \" \"+str(i)\n",
        "  output += \"</p><hr>\"\n",
        "    \n",
        "output = '<h3>Results:</h3>'+output\n",
        "display(HTML(output))\n",
        "#   print(sentences[i])\n",
        "#   print('\\n')\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Results:</h3><p style=\"font-family:verdana; font-size:110%;\">  JJ Espinoza Director | Data Science Twentieth Century Fox Los Angeles, CA, United States Advanced Solutions Lab, Google, 2018-2018 AI engineering in residence | Tensorflow | Deep Learning, CNN, RNN | Cloud ML Guest Lecturer | Econometrics, University of California, Los Angeles, 2012-2018 Frequent guest lecturer to 500+ students (4 times <b>a</b> year) at UCLA's undergraduate econometrics lab course. Topics of discussion have included: ARIMA models for retail sales prediction, instrumental variable regression to control for endogeneity in consumer product sales and TV advertising, optimization of TV marketing budgets using econometrics and linear programming optimization algorithms. Careers in econometrics. Director | Business Analytics, NBCUniversal Media, LLC, 2016-2017 Hands-on Machine Learning and Econometrics for ROI Modeling and content protection. Leading of team of data science managers, senior analyst and MBA interns. Frequent senior executive presentations (President of Film Studio, CMO of Cable division, General Counsel, EVP, etc) presentations in London, Philadelphia (Comcast), and New York. Project management, discretionary data & consulting budget. Hiring, coaching, and development of data science managers/ analyst. Manager | Data Science, The Walt Disney Company, 2013-2016 Analyst | Planning & Forecasting, Pacific Sunwear, 2010-2011 Research Assistant to Finance Professor, UCLA Anderson School of Management, 2007-2009 Adsense, Algorithm Design, Amazon Web Services (AWS), Analytics, Apache Spark, Artificial Intelligence, Big Data, Blogging, Business Analytics, Cloud Computing, Consulting, Data Visualization, Delegation, Digital Marketing, Econometrics, Economics, Email Marketing, Entrepreneurship, Facebook Marketing, Google Adwords, Google Analytics, Google Cloud Platform, Leadership, Linux, Machine Learning, Management, Management Consulting, Marketing, Marketing Mix Modeling, Mentoring, Microsoft PowerPoint, Project Management, Public Speaking, Python, <b>R,</b> SAS, Search Engine Marketing (SEM), Search Engine Optimization (SEO), Social Media Marketing, SQL, Start-ups, Statistical Modeling, Statistics, Strategic Partnerships, Strategic Planning, Tableau, User Interface Design, Web Analytics, Web Development, WordPress, SPSS, Time Series Analysis, Microsoft Excel, Microeconomics, Market Research, Retail Sales Analysis, Quantitative Analytics, Forecasting, Macroeconomics, VBA, Regression Analysis, Marketing Research, SAS programming, Quantitative Research, Econometric Modeling, Economic Research, Segmentation, Applied Mathematics, Financial Modeling, Financial Analysis, Business Intelligence, Film Distribution Master of Science - MS, Econometrics, California State University, Fullerton Bachelor of Science - BS, Mathematics/Applied Science, University of California, Los Angeles Bachelor of Science - BS, Economics, University of California, Los Angeles Associate of Arts - AA, Business Administration and Management, General, Cerritos College Associate of Science - AS, Mathematics and Computer Science, Cerritos College Associate of Arts - AA, Economics, Cerritos College</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Adam Owens Data Scientist, Revenue Snapchat, Inc. Los Angeles, CA, United States Marketing Data Analyst, Snapchat, Inc., 2017-2018 Build and manage Snap marketing database. Build data products and analytical reports for Snap marketing efforts. Data Analytics Instructor, Product School, 2017-2017 Instructing the Data Analytics for Managers evening course Data Strategy Consultant / Founder, Vault Economics, 2014-2017 - Strategic positioning of new products including marginal feature effect, barriers of adoption, and product valuation Projects: - Priceline.com Marketing Analytics - Business Strategy at Crewspark Data Scientist, Intern, WeWork, 2017-2017 Built RecSpace: - Constraint-based recommender for WeWork office spaces and products. - Python module built to recommend locations for website, email marketing, internal dashboards, and internal research. - Built simulation including stochastic sampling of historical WeWork tour data and simulated users - Currently uses Random Forest and Hierarchical Clusters models in the constrained recommendations. WeNewsFeed: - Researched and built <b>a</b> prototyped <b>a</b> parallelized newsfeed recommendation engine for WeWork members. - Built taxonomy schema for members, social interactions, and content. - Researched LDA, LSI, and HDP models later used in matrix factorization ranking. - Built <b>an</b> initial python prototype cosine similarity ranking for member and post content. Data Science, WeWork, 2017- Technical Consultant - Marketing Analytics, Priceline.com, 2015-2015 - Managing Priceline's keyword bidding algorithm for Google, Yahoo, and Bing search engines- Research and analysis for SEM marketing channels, campaign performance, bidding and grouping structures- Tech stack used: SQL, <b>R,</b> Python- Analysis performed: Linear regressions, GLS, Multivariate Regression Splines, XGBoost, Gradient Boosting, statistical inference, t-tests, other general statistics - Examined the bidding curve and recommended adjustments in various segments to correctly target ROI - Incorporated new geographic dimensions in the bidding algorithm - Stat packages: RStudio, Anaconda/Sypder, Excel, Statsplus, RazorSQL Investment Banking, Wells Fargo, 2010-2014 Corporate Finance &amp; Capital Markets:- Investment and Non-Investment Grade Debt Issuances- Debt Private Placement Origination &amp; Syndication- Loan Origination &amp; Syndication Corporate Financial Analyst, Citigroup, 2009-2009 - Analyzed the capital structure and balance sheet of potential clients in Citi’s Caribbean Financial Strategy Group Economics Research Assistant, Brigham Young University, 2009-2009 - Researched and developed <b>a</b> general equilibrium model for labor flows between Japan and Korea using Excel, STATA and MATLAB- Received <b>a</b> research notation in the Korea and the World Economy journal Research Assistant Intern, Center for Strategic Studies at the University of Jordan, 2006-2006 - Collected economic data and statistics <b>on</b> Palestine, Jordan, and Israel to study the economic impact of the Israeli/Palestinian Separation Wall <b>on</b> the Jordanian economy. The report was prepared for the Jordanian Government- Researched the impact of various taxes <b>on</b> Jordanian democracy and parliamentary elections Business Strategy, Crewspark, 2015-2015 - Lead <b>on</b> business strategy, finance, operations and marketing- Analyzed strategic product placement in the Big Data industry and formulated <b>a</b> plan for the the MVP- Developed the financial planning and analysis foundation, marketing plan, company valuation and investment raising strategy- Wrote the investor presentation, business plan and marketing pitch- Worked with data scientists and business professionals in product development- Quantified the strategic advantage of the Crewspark platform against major competitors - In the product development strategy team Accounting, Apache Spark, Bloomberg, Business Analysis, Business Intelligence, Business Strategy, Capital Markets, Corporate Finance, Data Analysis, Data Mining, Data Science, Design, Due Diligence, Econometric Modeling, Economics, Finance, Financial Analysis, Financial Modeling, Financial Services, Git, Hadoop, Investment Banking, Investments, Machine Learning, Marketing Strategy, Microsoft Excel, Microsoft Office, PowerPoint, Product Development, Programming, Python, <b>R,</b> SEM, SQL, Start-up Consulting, Statistics, Strategy, Valuation Certificate / Masters Degree, Computer Science, Columbia University in the City of New York, 2016-2019 BA, Economics, Brigham Young University, 2005-2009 Data Science Bootcamp, NYC Data Science Academy, 2016-2016 Classwork, Computer Science, The Johns Hopkins University, 2015-2015</p><hr><p style=\"font-family:verdana; font-size:110%;\">  Orestes Gonzalo Manzanilla-Salazar Chief Data Science Officer Oplus Blue Montreal, QC, Canada Consultant, Inelectra, S.A.C.A., 2008-2015 Consultant specializing in areas of Operations Research / Operations Management and SAP Applications, under <b>a</b> professional fees contract. Teacher & Researcher, Universidad Simón Bolívar, 2008-2015 Teacher & researcher in the areas of Operations Research, Data Mining and Production Managment. Business Intelligence Independent Consultant, Experian, 2014-2014 -Managed existing Business Intelligence personnel to optimize workload and reduce backlog. -Developed new BI studies and product ideas to be developed by the team. -Analyzed data patterns and statistics of over 25M registries of credit information. -Conducted analysis and developed standardized reports of business intelligence for client and internal use. Enterprise Systems & Processes' Engineer, Inelectra, S.A.C.A., 2005-2008 Enterprise processes optimization using techniques from the areas of Quality Management, Enterprise Engineering, and the use of ERP solutions (SAP). Help-Desk for Cross-Module ERP solutions, workflow implementations and web employee self-service applications. Leader of the process modelling group. Creator of <b>a</b> methodology for the construction of Process Definition Charts (technical process maps) from standarized procedural documentation. Numerical data analysis and scenario modelling. Design of efficiency & efficacy indicators. Performed statistical analysis to quantify the consequences of certain operational changes to the financial management processes. Data Entry Clerk, Restore Plc, 2015- Data entry clerk, collating & classifying highly confidential documents into various internal systems. The tasks involve checking and sensing documentation before passing it <b>on</b> to the relevant teams / systems. This work is done as part of <b>a</b> project for KPMG, related to bank & finance. Teaching Assistant, Universidad Simón Bolívar, 2004-2005 Laboratory Teacher of Operations Research, Chemical Processes, & Statistics. Member of the CESMa (Statistical & Mathematical Software Center). Organizational Processes` Engineering Analyst, Universidad Simón Bolívar, 2003-2004 * Wrote Enterprise Engineering`s techniques` manuals. Applied Process Definition Modelling standards in <b>an</b> Organizational Processes Engineering Project. * Developed Scenario-based analysis for the Human Resources Department. Algorithm Design, Algorithms, Analysis, Artificial Intelligence, AutoCAD, Business Intelligence, Business Process Analysis, Data Analysis, Data Mining, Decision Support, Design of Experiments, E-Learning, Engineering Statistics, English, ERP, Forecasting, LabVIEW, LaTeX, Linear Programming, Lotus Notes, Machine Learning, Mathematical Modeling, Mathematical Programming, Matlab, Microsoft Excel, Microsoft Excel Solver, Microsoft Office, Microsoft Visio, Modeling, Operations Research, Optimization, Optimization Models, Optimizations, Pattern Recognition, Process Modeling, Production Engineering, Programming, Project Management, Research, Scenario Analysis, Simulations, Social Networking, Spanish, Statistics, Systems Analysis, Systems Engineering, Teaching, Turbo Pascal, VBA, Wikis Doctor of Philosophy (PhD), Electrical Engineering, l'École Polytechnique de Montréal, 2016-2020 Master, Systems Engineering (Operations Research), Universidad Simón Bolívar, 2002-2005 Bachelor of Science (B.Sc.) (Honours), Production Engineering, Universidad Simón Bolívar, 1996-2002 1st Chi, Pakua Chuen, Taichi Chuan (Yang and Chen styles) & Chi Kung, Escuela Pachi Taichi Chuan Venezuela, 2003-2015 Bachiller, Ciencias, Unidad Educativa Colegio \"El Ángel\"</p><hr>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5OTsplHrXQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}